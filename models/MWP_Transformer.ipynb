{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2af472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import os.path\n",
    "import torchtext\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import pi\n",
    "from math import sqrt\n",
    "from glob import glob\n",
    "from torch import optim\n",
    "from torchtext import vocab\n",
    "from konlpy.tag import Mecab\n",
    "from torchinfo import summary\n",
    "from torch.nn import Transformer\n",
    "from torchtext.vocab import vocab\n",
    "from math import factorial as fact\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import json\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78303da",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(777)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = sorted(glob('../embeddings/*44*'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd708671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(name):\n",
    "    if name == 'fast':\n",
    "        que_emb= torchtext.vocab.Vectors(question[1])\n",
    "    elif name == 'glove':\n",
    "        que_emb = torchtext.vocab.Vectors(question[3])\n",
    "    elif name == 'cbow':\n",
    "        que_emb = torchtext.vocab.Vectors(question[0])\n",
    "    else:\n",
    "        que_emb = torchtext.vocab.Vectors(question[2])\n",
    "    \n",
    "    que_vocab = torchtext.vocab.vocab(que_emb.stoi, min_freq= 0, specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "    \n",
    "    vector_dic = {'que': que_emb} #{'que': que_emb, 'equ': equ_emb} # dictionary 형태로 해당 단어에 대한 벡터 값들 저장\n",
    "    \n",
    "    vocab_dic = {'que': que_vocab} #{'que' : que_vocab, 'equ' : equ_vocab} # 위와 같은 형태로 단어 사전 저장\n",
    "    \n",
    "    for vocab in ['que']:\n",
    "        a = torch.zeros(4,256, requires_grad = True)                                 ## 해당 스페션 토큰에 대한 임베딩 값을 만들어주기 위함\n",
    "        vector_dic[vocab].vectors= torch.cat([a, vector_dic[vocab].vectors], dim=0) #              concat 시켜 기존 벡터들에 스페셜 토큰의 임베딩 벡터를 만들어줌\n",
    "        vector_dic[vocab].stoi = dict(zip(vector_dic[vocab].stoi.keys(), map(lambda x:x[1]+4, vector_dic[vocab].stoi.items()))) # 스페셜 토큰이 추가 되었으니 기존 단어들의 인덱스를 뒤로 밀어줘야함\n",
    "        for i, j in enumerate([\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]):\n",
    "            vector_dic[vocab].stoi[j] = i                  # 사전훈련으로 임베딩 된 단어들은 special 토큰이 없기에 삽입, torchtext 상위 버전에서는 따로 함수  제공\n",
    "        vocab_dic[vocab].set_default_index(0)                    #여기서 0번 인덱스는 <unk>를 의미함\n",
    "    \n",
    "    return vocab_dic, vector_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56290484",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_vocab ,pre_emb = load_embedding('fast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16857eb0",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ef2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/train_44_type_150000.json', 'r', encoding = 'utf-8-sig') as json_:\n",
    "    json_data = json.load(json_)\n",
    "\n",
    "train_json= [[] for i in range(len(json_data))]\n",
    "\n",
    "for i in json_data:\n",
    "    for j in json_data[i]:\n",
    "        train_json[int(i)].append(json_data[i][j])\n",
    "        \n",
    "        \n",
    "\n",
    "with open('../data/val_44_type_30000.json', 'r', encoding = 'utf-8-sig') as json_:\n",
    "    json_data = json.load(json_)\n",
    "\n",
    "valid_json= [[] for i in range(len(json_data))]\n",
    "\n",
    "for i in json_data:\n",
    "    for j in json_data[i]:\n",
    "        valid_json[int(i)].append(json_data[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test_44_type_30000.json', 'r', encoding = 'utf-8-sig') as json_:\n",
    "    json_data = json.load(json_)\n",
    "\n",
    "test_json= [[] for i in range(len(json_data))]\n",
    "\n",
    "for i in json_data:\n",
    "    for j in json_data[i]:\n",
    "        test_json[int(i)].append(json_data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b58f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(df):\n",
    "    '''\n",
    "        Args:\n",
    "            df (DataFrame): 데이터 프레임, column 0 은 question, 1은 equation\n",
    "        \n",
    "        Returns:\n",
    "            que_length (int) : 문제를 토큰화 한 후 가장 긴 길이 반환\n",
    "            equ_length (int) :  식을 토큰화 한 후 가장 긴 길이 반환.\n",
    "    '''\n",
    "    length = 0\n",
    "    list_que = []\n",
    "    for i in range(len(df)):\n",
    "        list_que.append(len(que_tokenizer(df[i][0])))\n",
    "    que_length = max(list_que)\n",
    "    que_length += 2 ## eos 까지 붙는걸 계산\n",
    "    return que_length #,equ_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc1a93",
   "metadata": {},
   "source": [
    "## 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def que_tokenizer(sentence, is_stopwords = True):\n",
    "    '''\n",
    "        Args:\n",
    "            sentence (str) : 입력 문장\n",
    "    \n",
    "        Returns: \n",
    "            mecab.morphs(sentence) (list[str]) : 리스트에 형태소 단위로 분해됨.\n",
    "    '''\n",
    "    mecab = Mecab()\n",
    "    stopwords = ['은','?','일까요','십시오','입니까','인가요',\n",
    "             ',인지','한다면','가','다','에는','에서','기록', '순위',\n",
    "             '앉아', '줄로','.','시','오', '습니다','인지','한다','여라']\n",
    "    \n",
    "    if is_stopwords:\n",
    "        a = mecab.morphs(sentence)\n",
    "        a = [word for word in a if word not in stopwords]\n",
    "        return a\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return mecab.morphs(sentence) ## 형태소 기준으로 분해\n",
    "\n",
    "def yield_que_tokenizer(sentence, make_vocab = True):\n",
    "    '''\n",
    "        Args:\n",
    "            sentence (str) : 입력 문장\n",
    "    \n",
    "        Returns: \n",
    "            mecab.morphs(sentence) (list[str]) : 리스트에 형태소 단위로 분해됨.\n",
    "    '''\n",
    "    stopwords = ['은','?','일까요','십시오','입니까','인가요',\n",
    "             ',인지','한다면','가','다','에는','에서','기록', '순위',\n",
    "             '앉아', '줄로', '.','시','오', '습니다','인지','한다','여라']\n",
    "\n",
    "    mecab = Mecab()\n",
    "    for i in sentence:\n",
    "        a = mecab.morphs(i[0])\n",
    "        a = [word for word in a if word not in stopwords]\n",
    "        yield a   \n",
    "      \n",
    "                 \n",
    "\n",
    "def yield_equ_tokenizer(sentence):\n",
    "    '''\n",
    "        Args:\n",
    "            sentence (str) : 입력 문장\n",
    "    \n",
    "        Returns: \n",
    "            mecab.morphs(sentence) (list[str]) : 리스트에 형태소 단위로 분해됨.\n",
    "    '''\n",
    "    for i in sentence:\n",
    "        equ = re.sub(\"([()])\", r' \\1 ', i[1])\n",
    "        equ = re.sub(\"([\\+\\-\\*\\%\\//\\'\\[\\]\\,\\>\\<])\", r' \\1 ',equ)\n",
    "        yield equ.split()\n",
    "\n",
    "def equ_tokenizer(sentence):\n",
    "    '''\n",
    "        Args:\n",
    "            sentence (str) : 입력 문장\n",
    "    \n",
    "        Returns: \n",
    "            mecab.morphs(sentence) (list[str]) : 리스트에 형태소 단위로 분해됨.\n",
    "    '''\n",
    "    equ = re.sub(\"([()])\", r' \\1 ', sentence)\n",
    "    equ = re.sub(\"([\\+\\-\\*\\%\\//\\'\\[\\]\\,\\>\\<])\", r' \\1 ',equ)\n",
    "    return equ.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cca5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_len= max_length(train_json)\n",
    "valid_len = max_length(valid_json)\n",
    "test_len = 0\n",
    "que_len =max(que_len, valid_len, test_len)\n",
    "#que_len = 99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aef29",
   "metadata": {},
   "source": [
    "## 단어 사전 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "equ_vocab = build_vocab_from_iterator(yield_equ_tokenizer(train_json), specials = [\"<unk>\", \"<pad>\",\"<sos>\", \"<eos>\"], min_freq = 1)\n",
    "equ_vocab.set_default_index(equ_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6900f",
   "metadata": {},
   "source": [
    "## 포지셔널 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd34a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위치 인코딩을 위한 포지셔널 인코딩\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, max_len, dropout ):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size) ## shape = (1,256)\n",
    "        pos = torch.arange(0, max_len).reshape(max_len, 1) ## shape (maxlen, 1)\n",
    "        \n",
    "        pos_embedding = torch.zeros((max_len, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding) ## opimizer가 업데이트 하지 않음, 그러나 값은 존재, GPU연산 가능, 업데이트 하고싶지 않은 층\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx = 1) ## 단어사전 \n",
    "        self.emb_size = emb_size\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "    \n",
    "class pre_TokenEmbedding(nn.Module): ## pretrain_word embedding\n",
    "    def __init__(self, vocab, emb_size):\n",
    "        super(pre_TokenEmbedding,self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors, padding_idx=1, freeze= False)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 emb_size, \n",
    "                 nhead,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 max_len,\n",
    "                 dim_feedforward = 512, \n",
    "                 dropout = 0.4,\n",
    "                pretrain = False):\n",
    "        \n",
    "        self.pretrain = pretrain\n",
    "        super(Seq2SeqTransformer,self).__init__()\n",
    "        \n",
    "        self.transformer = Transformer(d_model = emb_size,\n",
    "                                      nhead = nhead,\n",
    "                                      num_encoder_layers = num_encoder_layers,\n",
    "                                      num_decoder_layers = num_decoder_layers,\n",
    "                                      dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "        \n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        \n",
    "        self.src_tok_emb = pre_TokenEmbedding(pre_emb['que'], emb_size) ##pre_TokenEmbedding  #TokenEmbedding(src_vocab_size, emb_size)\n",
    "        \n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "            \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, max_len, dropout = dropout)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        \n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        \n",
    "        return self.generator(outs)\n",
    "        \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)),\n",
    "                                       src_mask)\n",
    "    \n",
    "    def decoder(self,tgt, memory, tgt_mask):\n",
    "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)),\n",
    "                                       memory, tgt_mask)\n",
    "    \n",
    "    def ck_pretrain(self):\n",
    "        return self.pretrain\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd569b",
   "metadata": {},
   "source": [
    "## 마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2933fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz,sz), device = device ))== 1).T  ## 삼각행렬로 만든 뒤\n",
    "    mask = mask.float().masked_fill(mask==0, float('-inf')).masked_fill(mask == 1, float(0.0)) ## 0은 -inf 와 같은 작은 값을 줘서 \n",
    "    return mask\n",
    "\n",
    "def create_mask(src,tgt):\n",
    "    src_seq_len = src.shape[0]   ## sequence_length\n",
    "    tgt_seq_len = tgt.shape[0]   ## sequence_length\n",
    "    \n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len) ## 디코더에서 셀프 어텐션 할때 그 뒤를 못보게 마스킹\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device = device).type(torch.bool)\n",
    "    \n",
    "    src_padding_mask = (src == 1).T ## (33,128)이 input으로 들어오는데 왜 transpose를 하는지=> 패딩이 위쪽\n",
    "    tgt_padding_mask = (tgt == 1).T\n",
    "    return src_mask, tgt_mask,src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c956ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(pre_vocab['que']) ## vocab_transform[SRC_LANGUAGE] pre_vocab[SRC_LANGUAGE]\n",
    "TGT_VOCAB_SIZE = len(equ_vocab) ## vocab_transform[TGT_LANGUAGE] pre_vocab[TGT_LANGUAGE]\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, que_len, FFN_HID_DIM,\n",
    "                                 pretrain = False)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index = 1)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr = 5e-4, betas = (0.9,0.98), eps = 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82875e9",
   "metadata": {},
   "source": [
    "## Transformer collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([2]),\n",
    "                     torch.tensor(token_ids),\n",
    "                     torch.tensor([3])))\n",
    "text_transform = {}\n",
    "\n",
    "\n",
    "text_transform['que'] = sequential_transforms(que_tokenizer, pre_vocab['que'], tensor_transform)\n",
    "text_transform['equ'] = sequential_transforms(equ_tokenizer, equ_vocab, tensor_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch, ans_list = [], [], []\n",
    "    for src_sample, tgt_sample, ans in batch:\n",
    "        src_batch.append(text_transform['que'](src_sample))\n",
    "        tgt_batch.append(text_transform['equ'](tgt_sample))\n",
    "        ans_list.append(ans)\n",
    "\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, padding_value = 1.0)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value = 1.0)\n",
    "    \n",
    "    return src_batch, tgt_batch, ans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_json, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_fn, drop_last = True,num_workers = 3)\n",
    "valid_loader = DataLoader(valid_json, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_fn, drop_last = True,num_workers = 3)\n",
    "test_loader = DataLoader(test_json, batch_size = BATCH_SIZE, collate_fn = collate_fn, drop_last = True, num_workers = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35431302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, valid_loader):\n",
    "    '''\n",
    "        Args:\n",
    "            model (Module)\n",
    "            optimizer (Optimizer)\n",
    "            train_loader (Dataloader)\n",
    "            valid_loader (Dataloader)\n",
    "    '''\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    val_losses = 0\n",
    "\n",
    "    for src,tgt, _ in train_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        tgt_input = tgt[:-1, :]\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tgt_out = tgt[1:, :]\n",
    "        \n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    \n",
    "    for src, tgt, _ in valid_loader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            tgt_input = tgt[:-1, :]\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "            logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "            tgt_out = tgt[1:, :]\n",
    "\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "            val_losses += loss.item()\n",
    "\n",
    "    \n",
    "    return losses / len(train_loader) , val_losses / len(valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "    \n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1,src.size(1)).fill_(start_symbol).type(torch.long).to(device) #ys = orch.tensor([[q_vocab['<sos>'] for i in range(src.size(1))]], device = device) ## 처음 시작은 <sos> 토큰\n",
    "    for i in range(max_len):\n",
    "        memory = memory.to(device)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)))\n",
    "        out = model.decoder(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0,1) # (sequence ,batch, hidden)  -> (batch,sequence ,hidden)\n",
    "        prob = model.generator(out[:, -1]) ## 마지막 단어\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.unsqueeze(0)\n",
    "        ys = torch.cat([ys, next_word], dim = 0)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577af55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_bleu(model,testset):\n",
    "    total_bleu = 0\n",
    "    acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for loader_idx ,(src, tgt, ans) in enumerate(testset):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            ans = list(map(float, np.array(ans)))\n",
    "            target_length = tgt.size(0)\n",
    "            num_tokens = src.shape[0] ## sequnce_length\n",
    "            \n",
    "            src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool) \n",
    "            tgt_tokens = new_greedy_decode(model, src, src_mask, max_len = num_tokens, start_symbol=2) #.flatten()\n",
    " \n",
    "            decoded_words = []\n",
    "            \n",
    "            for batch_idx in range(src.size(1)):         ## 배치의 각 문장마다 돌면서 <eos> 토큰을 만나면 문장을 끊도록\n",
    "                a = tgt_tokens[:, batch_idx]\n",
    "                subwords = []\n",
    "                for sent_idx, k in enumerate(a):\n",
    "                    if a[sent_idx] == 3:\n",
    "                        subwords.append(\"<eos>\")\n",
    "                        subwords = (\" \".join(subwords)).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                        decoded_words.append(subwords)\n",
    "                        break\n",
    "                    else:\n",
    "                        subwords.append(equ_vocab.lookup_token(a[sent_idx])) ## index to word \n",
    "                        if len(subwords) == (len(tgt_tokens)):\n",
    "                            subwords.append(\"<eos>\")\n",
    "                            subwords = (\" \".join(subwords)).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                            decoded_words.append(subwords)\n",
    "                            break \n",
    "                            \n",
    "            #predict_tgt = \" \".join(equ_vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")            \n",
    "\n",
    "            for pred_idx in range(src.size(1)): ## 각 시퀀스를 돌면서 계산\n",
    "                predict_equ = decoded_words[pred_idx]\n",
    "                real_equ = (\" \".join(equ_vocab.lookup_tokens(tgt.T.tolist()[pred_idx]))).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                try:\n",
    "                    predict_ans = eval(predict_equ.replace('@', '//'))\n",
    "                    predict_ans = float('%.02f' % predict_ans)\n",
    "                except (SyntaxError, TypeError, ZeroDivisionError,ValueError, OverflowError):\n",
    "                    continue\n",
    "                \n",
    "                if predict_ans == ans[pred_idx]:\n",
    "                    acc +=1\n",
    "                    \n",
    "                \n",
    "                decoded_words[pred_idx] = decoded_words[pred_idx].split()\n",
    "                #decoded_words[pred_idx] = decoded_words[pred_idx].extend([\"<eos>\"])\n",
    "                decoded_words[pred_idx].extend([\"<eos>\"])\n",
    "                \n",
    "                real = real_equ.split()\n",
    "                #real = [real.extend([\"<eos>\"])]\n",
    "                real.extend([\"<eos>\"])\n",
    "                \n",
    "                bleu = sentence_bleu([real], decoded_words[pred_idx],smoothing_function =None) # SmoothingFunction().method6\n",
    "                total_bleu += bleu   ## 배치의 bleu score 저장 나중에 총 배치 개수로 나눠 줘야함\n",
    "                \n",
    "                if (loader_idx == len(testset)-1) and (batch_idx == src.size(1) - 1):\n",
    "                    \n",
    "                    print(f\"sentence {pred_idx}\" )\n",
    "                    print(\"문장형 문제 =\", (' '.join(pre_vocab['que'].lookup_tokens(src.T.tolist()[pred_idx]))).replace(\"<pad>\", \"\").replace(\"<eos>\",\"\"))\n",
    "                    print(f\"실제 수식  = \", (' '.join(real)))\n",
    "                    print(\"실제 정답\", ans[pred_idx])\n",
    "                    print(f\"예측 수식  = \", \" \".join(decoded_words[pred_idx]))\n",
    "                    print(\"예측 정답\", predict_ans)\n",
    "\n",
    "             \n",
    "        \n",
    "    return total_bleu / len(testset.dataset), acc / len(testset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = open('./transformer_loss/fast_train_losses_384','w')\n",
    "b = open('./transformer_loss/fast_bleu_score_384','w')\n",
    "v = open('./transformer_loss/fast_valid_losses_384','w')\n",
    "a = open('./transformer_loss/fast_accuracy_384','w')\n",
    "t.close()\n",
    "b.close()\n",
    "v.close()\n",
    "a.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "NUM_EPOCHS = 300 ## 50\n",
    "best_val_loss = None\n",
    "before_loss = 999999\n",
    "count = 0\n",
    "'''\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "bleu_scores = []\n",
    "acc_list = []\n",
    "'''\n",
    "\n",
    "for epoch in range(1, 401):\n",
    "    t = open('./transformer_loss/fast_train_losses_384','a')\n",
    "    b = open('./transformer_loss/fast_bleu_score_384','a')\n",
    "    v = open('./transformer_loss/fast_valid_losses_384','a')\n",
    "    a = open('./transformer_loss/fast_accuracy_384','a')\n",
    "    try:\n",
    "        start_time = timer()\n",
    "        train_loss, val_loss= train_epoch(transformer, optimizer, train_loader, valid_loader)\n",
    "        end_time = timer()\n",
    "        bleu_score, acc = translate_bleu(transformer, valid_loader)\n",
    "        valid_end_time = timer()\n",
    "        \n",
    "        t.write(str(train_loss) + '\\n')\n",
    "        b.write(str(bleu_score) + '\\n')\n",
    "        v.write(str(val_loss) + '\\n')\n",
    "        a.write(str(acc) + '\\n')\n",
    "    finally:\n",
    "        t.close()\n",
    "        b.close()\n",
    "        v.close()\n",
    "        a.close()\n",
    "            \n",
    "        \n",
    "    print(f\"EPOCH : {epoch}, Train Loss : {train_loss}, Val Loss: {val_loss}, Val_Bleu_Score :{bleu_score}, Val_acc: {acc} \\\n",
    "train_time : {(end_time - start_time):.3f}s, valid_time :{(valid_end_time -end_time)}s\")\n",
    "\n",
    "    \n",
    "    before_loss = val_loss\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "\n",
    "        \n",
    "        torch.save(transformer.state_dict(), '../models/transformer_fasttext.pt')\n",
    "        best_val_loss = val_loss\n",
    "        count = 0\n",
    "        \n",
    "    else:\n",
    "        count += 1\n",
    "        if count == 20:\n",
    "            print(\"overfitting\")\n",
    "            break\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331336a",
   "metadata": {},
   "source": [
    "## 모델 평가 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_T = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, que_len, FFN_HID_DIM,\n",
    "                                 pretrain = False)\n",
    "transformer_T = transformer_T.to(device)\n",
    "transformer_T.load_state_dict(torch.load('../saved_weights/Transformer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74909907",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(\"../data/math_test_1000\" , header=None)\n",
    "\n",
    "\n",
    "def data_format(df, is_stopwords = False):\n",
    "    '''\n",
    "        Args:\n",
    "            df (DataFrame) : 데이터프레임 csv\n",
    "        \n",
    "        Returns:\n",
    "            list_ (list): 데이터를 받아서 ['문장', '라벨'] 로 연결\n",
    "        \n",
    "            ex) list_= [['민혁이가 잰 직사각형의 둘레는 48m이고, 직사각형의 세로는 가로 길이의 2배 입니다. 세로는 몇 m인지 구하세요.', '((48 / 2) / (2 + 1)) * 2'],\n",
    "    '''\n",
    "    list_ = [] \n",
    "    for i in range(len(df[0])):\n",
    "        list_.append([df[0][i]])\n",
    "    for i in range(len(df[0])):    \n",
    "        list_[i].append(df[1][i])\n",
    "    for i in range(len(df[0])):\n",
    "        list_[i].append(df[2][i])\n",
    "        \n",
    "    \n",
    "    return list_\n",
    "\n",
    "test_pairs = data_format(test_csv)\n",
    "\n",
    "real_loader = DataLoader(test_pairs, batch_size = 100, shuffle = False, collate_fn = collate_fn, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import CharErrorRate\n",
    "from torchmetrics.text import WordErrorRate\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "\n",
    "wer = WordErrorRate()\n",
    "cer = CharErrorRate()\n",
    "rouge = ROUGEScore(rouge_keys = ('rougeL'))['rougeL_fmeasure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_translate_bleu(model,testset):\n",
    "    total_bleu = 0\n",
    "    bleu_1gram = 0\n",
    "    bleu_2gram = 0\n",
    "    bleu_3gram = 0\n",
    "    total_cer = 0\n",
    "    total_rouge = 0\n",
    "    acc = 0\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    ### 이하는 나중에 지워도 됨\n",
    "    new_bleu_acc = 0\n",
    "    correct = 0\n",
    "    data_index = []\n",
    "    with torch.no_grad():\n",
    "        for loader_idx ,(src, tgt, ans) in enumerate(tqdm(testset)):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            ans = list(map(float, np.array(ans)))\n",
    "            target_length = tgt.size(0)\n",
    "            num_tokens = src.shape[0] ## sequnce_length\n",
    "            \n",
    "            src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool) \n",
    "            tgt_tokens = new_greedy_decode(model, src, src_mask, max_len = num_tokens+2, start_symbol=2)\n",
    " \n",
    "            decoded_words = []\n",
    "            \n",
    "            for batch_idx in range(src.size(1)):         ## 배치의 각 문장마다 돌면서 <eos> 토큰을 만나면 문장을 끊도록\n",
    "                a = tgt_tokens[:, batch_idx]\n",
    "                subwords = []\n",
    "                for sent_idx, k in enumerate(a):\n",
    "                    if a[sent_idx] == 3:\n",
    "                        subwords.append(\"<eos>\")\n",
    "                        subwords = (\" \".join(subwords)).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                        decoded_words.append(subwords)\n",
    "                        break\n",
    "                    else:\n",
    "                        subwords.append(equ_vocab.lookup_token(a[sent_idx])) ## index to word \n",
    "                        if len(subwords) == (len(tgt_tokens)):\n",
    "                            subwords.append(\"<eos>\")\n",
    "                            subwords = (\" \".join(subwords)).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                            decoded_words.append(subwords)\n",
    "                            break \n",
    "                            \n",
    "            for pred_idx in range(src.size(1)): ## 각 시퀀스를 돌면서 계산\n",
    "                predict_equ = decoded_words[pred_idx]\n",
    "                real_equ = (\" \".join(equ_vocab.lookup_tokens(tgt.T.tolist()[pred_idx]))).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                try:\n",
    "                    predict_ans = eval(predict_equ.replace('@', '//'))\n",
    "                    predict_ans = float('%.02f' % predict_ans)\n",
    "                except (SyntaxError, TypeError, ZeroDivisionError,ValueError, OverflowError):\n",
    "                    continue\n",
    "                \n",
    "                if predict_ans == ans[pred_idx]:\n",
    "                    acc +=1\n",
    "                    \n",
    "                \n",
    "                decoded_words[pred_idx] = decoded_words[pred_idx].split()\n",
    "                decoded_words[pred_idx].extend([\"<eos>\"])\n",
    "                \n",
    "                real = real_equ.split()\n",
    "                real.extend([\"<eos>\"])\n",
    "                \n",
    "                total_bleu += sentence_bleu([real], decoded_words[pred_idx]) # SmoothingFunction().method6\n",
    "                bleu_1gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(1.0)])\n",
    "                bleu_2gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(0.5,0.5)])\n",
    "                bleu_3gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(0.333,0.333,0.334)])\n",
    "                \n",
    "                \n",
    "                total_cer += cer([\" \".join(decoded_words[pred_idx])],[' '.join(real)]).item()\n",
    "                total_rouge += rouge([\" \".join(decoded_words[pred_idx])], [' '.join(real)]).item()\n",
    "                \n",
    "                \n",
    "                if (loader_idx == len(testset)-1) and (batch_idx == src.size(1) - 1):\n",
    "                    \n",
    "                    print(f\"sentence {pred_idx}\" )\n",
    "                    print(\"문장형 문제 =\", (' '.join(pre_vocab['que'].lookup_tokens(src.T.tolist()[pred_idx]))).replace(\"<pad>\", \"\").replace(\"<eos>\",\"\"))\n",
    "                    print(f\"실제 수식  = \", (' '.join(real)))\n",
    "                    print(\"실제 정답\", ans[pred_idx])\n",
    "                    print(f\"예측 수식  = \", \" \".join(decoded_words[pred_idx]))\n",
    "                    print(\"예측 정답\", predict_ans)\n",
    "\n",
    "             \n",
    "        \n",
    "    return total_bleu / (len(testset) * BATCH_SIZE), acc / (len(testset) * BATCH_SIZE), bleu_1gram / (len(testset) * BATCH_SIZE), bleu_2gram / (len(testset) * BATCH_SIZE), bleu_3gram / (len(testset) * BATCH_SIZE), total_cer / (len(testset) * BATCH_SIZE),total_rouge / (len(testset) * BATCH_SIZE)\n",
    "                    \n",
    "                \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score, acc, bleu_1, bleu_2, bleu_3, all_cer, all_rouge = eval_translate_bleu(transformer_T, test_loader)\n",
    "print(bleu_score, acc, bleu_1, bleu_2, bleu_3, all_cer, all_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_translate_bleu(model,testset):\n",
    "    total_bleu = 0\n",
    "    bleu_1gram = 0\n",
    "    bleu_2gram = 0\n",
    "    bleu_3gram = 0\n",
    "    total_cer = 0\n",
    "    total_rouge = 0\n",
    "    acc = 0\n",
    "    error_count = 0\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    ### 이하는 나중에 지워도 됨\n",
    "    new_bleu_acc = 0\n",
    "    correct = 0\n",
    "    data_index = []\n",
    "    with torch.no_grad():\n",
    "        for loader_idx ,(src, tgt, ans) in enumerate(testset):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            ans = list(map(float, np.array(ans)))\n",
    "            target_length = tgt.size(0)\n",
    "            num_tokens = src.shape[0] ## sequnce_length\n",
    "            \n",
    "            src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool) \n",
    "            tgt_tokens = new_greedy_decode(model, src, src_mask, max_len = num_tokens+2, start_symbol=2)\n",
    " \n",
    "            decoded_words = []\n",
    "            \n",
    "            for batch_idx in range(src.size(1)):         ## 배치의 각 문장마다 돌면서 <eos> 토큰을 만나면 문장을 끊도록\n",
    "                a = tgt_tokens[:, batch_idx]\n",
    "                subwords = []\n",
    "                for sent_idx, k in enumerate(a):\n",
    "                    if a[sent_idx] == 3:\n",
    "                        subwords.append(\"<eos>\")\n",
    "                        subwords = (\" \".join(subwords)).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                        decoded_words.append(subwords)\n",
    "                        break\n",
    "                    else:\n",
    "                        subwords.append(equ_vocab.lookup_token(a[sent_idx])) ## index to word \n",
    "                        if len(subwords) == (len(tgt_tokens)):\n",
    "                            subwords.append(\"<eos>\")\n",
    "                            subwords = (\" \".join(subwords)).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                            decoded_words.append(subwords)\n",
    "                            break \n",
    "                            \n",
    "            for pred_idx in range(src.size(1)): ## 각 시퀀스를 돌면서 계산\n",
    "                predict_equ = decoded_words[pred_idx]\n",
    "                real_equ = (\" \".join(equ_vocab.lookup_tokens(tgt.T.tolist()[pred_idx]))).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "                try:\n",
    "                    predict_ans = eval(predict_equ.replace('@', '//'))\n",
    "                    predict_ans = float('%.02f' % predict_ans)\n",
    "                except (SyntaxError, TypeError, ZeroDivisionError,ValueError, OverflowError):\n",
    "                    continue\n",
    "                \n",
    "                if predict_ans == ans[pred_idx]:\n",
    "                    acc +=1\n",
    "                    \n",
    "                \n",
    "                decoded_words[pred_idx] = decoded_words[pred_idx].split()\n",
    "                decoded_words[pred_idx].extend([\"<eos>\"])\n",
    "                \n",
    "                real = real_equ.split()\n",
    "                real.extend([\"<eos>\"])\n",
    "                \n",
    "                if ans[pred_idx] == predict_ans:\n",
    "                    correct += 1\n",
    "                    print(\"***********정답******************\")\n",
    "                    print(f\"데이터 몇번째 :{loader_idx * 10 + pred_idx +1} \") ## 10 = real_dataloader batch size\n",
    "                    data_index.append(loader_idx * 10 + pred_idx +1)\n",
    "                    print(\"문장형 문제 =\", (' '.join(pre_vocab['que'].lookup_tokens(src.T.tolist()[pred_idx]))).replace(\"<pad>\", \"\").replace(\"<eos>\",\"\"))\n",
    "                    print(f\"실제 수식  = \", (' '.join(real)))\n",
    "                    print(f\"예측 수식  = \", \" \".join(decoded_words[pred_idx]))\n",
    "                    print(\"실제 정답\", ans[pred_idx])\n",
    "                    print(\"예측 정답\", predict_ans)\n",
    "                    bleu_score_acc = sentence_bleu([real], decoded_words[pred_idx],smoothing_function =None)\n",
    "                    \n",
    "                    bleu_1gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(1.0)])\n",
    "                    bleu_2gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(0.5,0.5)])\n",
    "                    bleu_3gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(0.333,0.333,0.334)])\n",
    "                    total_cer += cer([\" \".join(decoded_words[pred_idx])],[' '.join(real)]).item()\n",
    "                    total_rouge += rouge([\" \".join(decoded_words[pred_idx])], [' '.join(real)]).item()                  \n",
    "                    #total_cer += cer(real, decoded_words[pred_idx]).item()\n",
    "                    new_bleu_acc += bleu_score_acc\n",
    "                    \n",
    "                    print(\"*********************************\")\n",
    "       \n",
    "                else:\n",
    "                    if error_count < 210:\n",
    "                        print(\"^^^^^^^^^^오답^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        error_count += 1\n",
    "                        print(\"문장형 문제 =\", (' '.join(pre_vocab['que'].lookup_tokens(src.T.tolist()[pred_idx]))).replace(\"<pad>\", \"\").replace(\"<eos>\",\"\"))\n",
    "                        print(f\"실제 수식  = \", (' '.join(real)))\n",
    "                        print(f\"예측 수식  = \", \" \".join(decoded_words[pred_idx]))\n",
    "                        print(\"실제 정답\", ans[pred_idx])\n",
    "                        print(\"예측 정답\", predict_ans)\n",
    "                        bleu_score_acc = sentence_bleu([real], decoded_words[pred_idx],smoothing_function =None)\n",
    "\n",
    "                        bleu_1gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(1.0)])\n",
    "                        bleu_2gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(0.5,0.5)])\n",
    "                        bleu_3gram += sentence_bleu([real], decoded_words[pred_idx], weights = [(0.333,0.333,0.334)])\n",
    "                        total_cer += cer([\" \".join(decoded_words[pred_idx])],[' '.join(real)]).item()\n",
    "                        total_rouge += rouge([\" \".join(decoded_words[pred_idx])], [' '.join(real)]).item()\n",
    "                        new_bleu_acc += bleu_score_acc\n",
    "    \n",
    "        print(f\"total 300/ correct:{correct/311} \")\n",
    "        print(f\"correct : {correct}\")\n",
    "        print(f\"data index\",data_index)\n",
    "        print(f\"total 300 bleu :{new_bleu_acc/ 311}\")\n",
    "        print(f\"total 300 bleu_1 :{bleu_1gram/ 311}\")\n",
    "        print(f\"total 300 bleu_2 :{bleu_2gram/ 311}\")\n",
    "        print(f\"total 300 bleu_3:{bleu_3gram/ 311}\")\n",
    "        print(f\"total cer :{total_cer}\")\n",
    "        print(f\"total 300 cer :{total_cer/ 311}\")\n",
    "        print(f\"total 300 rouge :{total_rouge/ 311}\")\n",
    "\n",
    "        print(f\"error count :{error_count}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2d6ef",
   "metadata": {},
   "source": [
    "## 실제 데이터로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ee49a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_score = timer()\n",
    "new_translate_bleu(transformer_T, real_loader)\n",
    "e_score = timer()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
