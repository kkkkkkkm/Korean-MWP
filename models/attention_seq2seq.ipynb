{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import os.path\n",
    "import torchtext\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from math import pi\n",
    "from math import sqrt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torchtext import vocab\n",
    "from konlpy.tag import Mecab\n",
    "from torchinfo import summary\n",
    "from torchtext.vocab import vocab\n",
    "from math import factorial as fact\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#from torchsummary import summary as summary\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071478e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "EPOCHS = 200\n",
    "batch_size = 1024\n",
    "num_layer = 3\n",
    "\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca662b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(777)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81882fba",
   "metadata": {},
   "source": [
    "## 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = sorted(glob('../embeddings/*44*'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fdbfb",
   "metadata": {},
   "source": [
    "## 임베딩 벡터 로드 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(name):\n",
    "    if name == 'fast':\n",
    "        que_emb= torchtext.vocab.Vectors(question[1])\n",
    "        #equ_emb = torchtext.vocab.Vectors(equation[1])\n",
    "    elif name == 'glove':\n",
    "        que_emb = torchtext.vocab.Vectors(question[3])\n",
    "        #equ_emb = torchtext.vocab.Vectors(equation[3])\n",
    "    elif name == 'cbow':\n",
    "        que_emb = torchtext.vocab.Vectors(question[0])\n",
    "        #equ_emb = torchtext.vocab.Vectors(equation[0])\n",
    "    else:\n",
    "        que_emb = torchtext.vocab.Vectors(question[2])\n",
    "        #equ_emb = torchtext.vocab.Vectors(equation[2])\n",
    "    \n",
    "    que_vocab = torchtext.vocab.vocab(que_emb.stoi, min_freq= 0, specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "    #equ_vocab = torchtext.vocab.vocab(equ_emb.stoi, min_freq= 0)\n",
    "    \n",
    "    vector_dic = {'que': que_emb} #{'que': que_emb, 'equ': equ_emb} # dictionary 형태로 해당 단어에 대한 벡터 값들 저장\n",
    "    \n",
    "    vocab_dic = {'que': que_vocab} #{'que' : que_vocab, 'equ' : equ_vocab} # 위와 같은 형태로 단어 사전 저장\n",
    "    \n",
    "    for vocab in ['que']:\n",
    "        a = torch.zeros(4,256, requires_grad = True)                                 ## 해당 스페션 토큰에 대한 임베딩 값을 만들어주기 위함\n",
    "        vector_dic[vocab].vectors= torch.cat([a, vector_dic[vocab].vectors], dim=0) #              concat 시켜 기존 벡터들에 스페셜 토큰의 임베딩 벡터를 만들어줌\n",
    "        vector_dic[vocab].stoi = dict(zip(vector_dic[vocab].stoi.keys(), map(lambda x:x[1]+4, vector_dic[vocab].stoi.items()))) # 스페셜 토큰이 추가 되었으니 기존 단어들의 인덱스를 뒤로 밀어줘야함\n",
    "        for i, j in enumerate([\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]):\n",
    "            vector_dic[vocab].stoi[j] = i                  # 사전훈련으로 임베딩 된 단어들은 special 토큰이 없기에 삽입, torchtext 상위 버전에서는 따로 함수  제공\n",
    "        vocab_dic[vocab].set_default_index(0)                    #여기서 0번 인덱스는 <unk>를 의미함\n",
    "    \n",
    "    return vocab_dic, vector_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9dfcf4",
   "metadata": {},
   "source": [
    "## 임베딩 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_vocab ,pre_emb = load_embedding('fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_vocab['que']['일까']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a81862",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5448045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('../data/train_44_type_150000.json', 'r', encoding = 'utf-8-sig') as json_:\n",
    "    json_data = json.load(json_)\n",
    "\n",
    "train_json= [[] for i in range(len(json_data))]\n",
    "\n",
    "for i in json_data:\n",
    "    for j in json_data[i]:\n",
    "        train_json[int(i)].append(json_data[i][j])\n",
    "        \n",
    "        \n",
    "\n",
    "with open('../data/val_44_type_30000.json', 'r', encoding = 'utf-8-sig') as json_:\n",
    "    json_data = json.load(json_)\n",
    "\n",
    "valid_json= [[] for i in range(len(json_data))]\n",
    "\n",
    "for i in json_data:\n",
    "    for j in json_data[i]:\n",
    "        valid_json[int(i)].append(json_data[i][j])\n",
    "        \n",
    "with open('../data/test_44_type_30000.json', 'r', encoding = 'utf-8-sig') as json_:\n",
    "    json_data = json.load(json_)\n",
    "\n",
    "test_json= [[] for i in range(len(json_data))]\n",
    "\n",
    "for i in json_data:\n",
    "    for j in json_data[i]:\n",
    "        test_json[int(i)].append(json_data[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea540b18",
   "metadata": {},
   "source": [
    "## 문제, 수식 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c75f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def que_tokenizer(sentence, is_stopwords = False):\n",
    "    '''\n",
    "        Args:\n",
    "            sentence (str) : 입력 문장\n",
    "    \n",
    "        Returns: \n",
    "            mecab.morphs(sentence) (list[str]) : 리스트에 형태소 단위로 분해됨.\n",
    "    '''\n",
    "    mecab = Mecab()\n",
    "    stopwords = ['은','?','일까요','십시오','입니까','인가요',\n",
    "             ',인지','한다면','가','다','에는','에서','기록', '순위',\n",
    "             '앉아', '줄로', '.','시','오', '습니다','인지','한다','여라']\n",
    "    \n",
    "    if is_stopwords:\n",
    "        a = mecab.morphs(sentence)\n",
    "        a = [word for word in a if word not in stopwords]\n",
    "        return a\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return mecab.morphs(sentence) ## 형태소 기준으로 분해\n",
    "\n",
    "def yield_que_tokenizer(sentence, make_vocab = False):\n",
    "    '''\n",
    "        Args:\n",
    "            sentence (str) : 입력 문장\n",
    "    \n",
    "        Returns: \n",
    "            mecab.morphs(sentence) (list[str]) : 리스트에 형태소 단위로 분해됨.\n",
    "    '''\n",
    "    stopwords = ['은','?','일까요','십시오','입니까','인가요',\n",
    "             ',인지','한다면','가','다','에는','에서','기록', '순위',\n",
    "             '앉아', '줄로', '.','시','오', '습니다','인지','한다','여라']\n",
    "\n",
    "    mecab = Mecab()\n",
    "    for i in sentence:\n",
    "        a = mecab.morphs(i[0])\n",
    "        a = [word for word in a if word not in stopwords]\n",
    "        yield a   \n",
    "      \n",
    "                 \n",
    "\n",
    "def yield_equ_tokenizer(sentence, make_vocab= False):\n",
    "    '''\n",
    "        Args:\n",
    "            sentence (str) : 입력 문장\n",
    "    \n",
    "        Returns: \n",
    "            mecab.morphs(sentence) (list[str]) : 리스트에 형태소 단위로 분해됨.\n",
    "    '''\n",
    "    for i in sentence:\n",
    "        equ = re.sub(\"([()])\", r' \\1 ', i[1])\n",
    "        equ = re.sub(\"([\\+\\-\\*\\%\\//\\'\\[\\]\\,\\>\\<])\", r' \\1 ',equ)\n",
    "        yield equ.split()\n",
    "\n",
    "def equ_tokenizer(sentence):\n",
    "    '''\n",
    "        Args:\n",
    "            sentence (str) : 입력 문장\n",
    "    \n",
    "        Returns: \n",
    "            mecab.morphs(sentence) (list[str]) : 리스트에 형태소 단위로 분해됨.\n",
    "    '''\n",
    "    equ = re.sub(\"([()])\", r' \\1 ', sentence)\n",
    "    equ = re.sub(\"([\\+\\-\\*\\%\\//\\'\\[\\]\\,\\>\\<])\", r' \\1 ',equ)\n",
    "    return equ.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17763f3b",
   "metadata": {},
   "source": [
    "## 단어장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "equ_vocab = build_vocab_from_iterator(yield_equ_tokenizer(train_json, make_vocab=True), specials = [\"<unk>\", \"<pad>\",\"<sos>\", \"<eos>\"], min_freq = 1)\n",
    "equ_vocab.set_default_index(equ_vocab['<unk>'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb37e35",
   "metadata": {},
   "source": [
    "## 3. 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_transform(sentence, is_que = False):\n",
    "    \n",
    "    '''\n",
    "        문장을 인덱스로 변환하는 함수\n",
    "        \n",
    "        Args:\n",
    "            sentence (str): 입력 문장.\n",
    "        \n",
    "        Returns:\n",
    "            text2index (list[(int)]) : 문장을 토큰화 하고 해당 하는 토큰의 인덱스 반환.    \n",
    "    '''\n",
    "    if is_que == True:\n",
    "        vocab = pre_vocab['que'] ## 수학문제 단어장\n",
    "        text2index = [vocab[token] for token in que_tokenizer(sentence, True)] + [vocab['<eos>']] ## Stopword 설정\n",
    "    else:\n",
    "        vocab = equ_vocab ## 수식 단어장\n",
    "        text2index = [vocab[token] for token in equ_tokenizer(sentence)] + [vocab['<eos>']]\n",
    "        \n",
    "    return text2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(df):\n",
    "    '''\n",
    "        Args:\n",
    "            df (DataFrame): 데이터 프레임, column 0 은 question, 1은 equation\n",
    "        \n",
    "        Returns:\n",
    "            que_length (int) : 문제를 토큰화 한 후 가장 긴 길이 반환\n",
    "            equ_length (int) :  식을 토큰화 한 후 가장 긴 길이 반환.\n",
    "    '''\n",
    "    length = 0\n",
    "    list_que = []\n",
    "    for i in range(len(df)):\n",
    "        list_que.append(len(que_tokenizer(df[i][0], True)))\n",
    "    que_length = max(list_que)\n",
    "    que_length += 1 ## eos 까지 붙는걸 계산\n",
    "    return que_length #,equ_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdaacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def new_collate_batch(batch):\n",
    "    '''\n",
    "        배치로 묶어주는 함수 DataLoader를 쓸 때 사용\n",
    "    '''\n",
    "    src_list, target_list, ans_list = [],[], []\n",
    "    for que, equ, ans in batch:\n",
    "        processed_src = torch.tensor(text_transform(que, is_que = True))\n",
    "        processed_tar= torch.tensor(text_transform(equ))\n",
    "        src_list.append(processed_src)\n",
    "        target_list.append(processed_tar)\n",
    "        ans_list.append(ans)\n",
    "        \n",
    "    pad_src_list = pad_sequence(src_list, padding_value = 1.0)\n",
    "    pad_target_list = pad_sequence(target_list, padding_value = 1.0) ## [sequence, batch] 형태로 만들어주는 pad_sequence, padding은 <pad>의 인덱스 번호 1로 설정\n",
    "    return pad_src_list, pad_target_list, ans_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92bd8ab",
   "metadata": {},
   "source": [
    "## 데이터 로더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df36abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_json, batch_size = batch_size, shuffle = True, collate_fn = new_collate_batch, drop_last = True)\n",
    "valid_loader = DataLoader(valid_json, batch_size = batch_size, shuffle = True, collate_fn = new_collate_batch, drop_last = True)\n",
    "test_loader = DataLoader(test_json, batch_size = batch_size, shuffle = False, collate_fn = new_collate_batch, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf452357",
   "metadata": {},
   "source": [
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a507b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_vocab, hidden_size ,batch_size ,num_layer, pretrain = False):\n",
    "        '''\n",
    "            input_vocab : 입력 언어 단어장 크기\n",
    "            hidden_size : 임베딩 층 히든 크기\n",
    "            batch_size (int) : 배치 크기\n",
    "            num_layer (int) : RNN 레이어 층 개수\n",
    "            pretrain (bool) : 사전학습 시킨 단어 임베딩을 사용할 건지 안할 건지\n",
    "        '''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch = batch_size\n",
    "        self.n_layer = num_layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers = self.n_layer)\n",
    "        self.pretrain = pretrain\n",
    "        \n",
    "        \n",
    "        if pretrain:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pre_emb['que'].vectors, padding_idx=1, freeze = False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_vocab, hidden_size, padding_idx = 1)\n",
    "            \n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,self.batch,-1) ## [sequence_length, batch, hidden_size]\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)  \n",
    "        return output, hidden # output = [sequence_length, batch, hidden_size] , hidden_out = [D*num_layer, batch, hidden_size]\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layer, self.batch, self.hidden_size, device = device) ## hidden_in [D*num_layer, batch, hidden_size]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    '''\n",
    "            Args:\n",
    "                output_size (int) : 출력 언어 단어 사전 크기, 여기선 식 임베딩 단어장 크기\n",
    "                hidden_size (int) : embedding layer hidden size\n",
    "                max_length (int) : 문장형 문제 중 문장 중 가장 긴 문장 값\n",
    "                batch_size (int) : 배치 크기\n",
    "                num_layer (int) : RNN 레이어 층 개수\n",
    "                dropout_p (float) : 드랍아웃 확률\n",
    "                pretrain (bool) : 사전학습 시킨 단어 임베딩을 사용할 건지 안할 건지\n",
    "            \n",
    "        '''\n",
    "    \n",
    "    def __init__(self, output_size, hidden_size, max_length, batch_size, num_layer, dropout_p = 0.4, pretrain = False):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.n_layer = num_layer\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length) ##concat, 2개의 히든을 받고 문장의 최대 길이 만큼 반환\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "    \n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, num_layers = self.n_layer)\n",
    "        self.classifier = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "                        \n",
    "        if pretrain:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pre_emb['equ'].vectors, padding_idx=1, freeze = False) #eng\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(output_size, hidden_size, padding_idx = 1)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        '''\n",
    "        input: input length [해당 문장 길이, 배치]\n",
    "        hidden: encoder hidden [레이어, 배치, 히든]\n",
    "        encoder_outputs : 어텐션 계산을 위한 인코더 히든값 encoder_outputs [배치, 최대 길이, 히든]\n",
    "            \n",
    "        '''\n",
    "        embedded = self.embedding(input).view(1,self.batch,-1) ## [1,128,hidden] (input_length, batch_size, hidden_size) \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        gru_out, hidden = self.gru(embedded, hidden) ## gru_out = [1, batch , hidden]\n",
    "        \n",
    "        permute_gru = gru_out.permute(1,0,2) # [batch,1,hidden]\n",
    "        \n",
    "        out = self.fc(permute_gru) # out = [batch,hidden]\n",
    "        \n",
    "        #out = out.unsqueeze(1)  # out = [batch,1,hidden]\n",
    "        \n",
    "        permute_outputs = encoder_outputs.permute(0,2,1) # length 와 hidden 위치를 바꿔줌\n",
    "         \n",
    "        alignment_scores= out.bmm(permute_outputs) #[batch, 1 , hidden] @ [batch ,hidden, length] = [batch, 1 , length] => attn score\n",
    "        \n",
    "        attn_weights = F.softmax(alignment_scores, dim=2) # [batch,1, length]\n",
    "        \n",
    "        context_vector = torch.bmm(attn_weights,encoder_outputs) ## [batch, 1, hidden]\n",
    "        \n",
    "        output = torch.cat((permute_gru, context_vector),dim = 2) #[batch, 1 , hidden * 2]\n",
    "        \n",
    "        output = output.squeeze() # batch, hidden * 2\n",
    "        \n",
    "        output = F.log_softmax(self.classifier(output), dim=1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layer, self.batch, self.hidden_size, device=device) #layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.3\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    '''\n",
    "        Args:\n",
    "            input_tensor (tensor) : [시퀀스, 배치]\n",
    "            target_tensor (tensor): [시퀀스, 배치]\n",
    "            encoder (Module) : 인코더 모델\n",
    "            decoder (Module) : 디코더 모델\n",
    "            encoder_optimizer (optimizer)  \n",
    "            decoder_optimizer (optimizer) \n",
    "            criterion (Loss)\n",
    "            max_length : 인코더 입력 문장 최대 길이\n",
    "            \n",
    "        Returns:\n",
    "            loss.item() / target_length (float) : target_length 만큼 loss 값을 더해 줘서 length로 나눠줌 \n",
    "    '''\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden() #[layer, batch, hidden_size]\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)  \n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(input_tensor.size(1), max_length, encoder.hidden_size, device = device) ## 어텐션 용, [batch, sequence_length, hidden]\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):  #한 글자씩 들어감  #### 한 개 씩 말고 그냥 배치 전체로 돌리는것도 고려\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) ## [1, 배치, 히든] \n",
    "        encoder_outputs[:, ei, :] = encoder_output[0,:] #[128, ei, 256] \n",
    "    \n",
    "\n",
    "        \n",
    "    if encoder.pretrain:\n",
    "        q_vocab = pre_vocab['que']\n",
    "        e_vocab = equ_vocab\n",
    "    else:\n",
    "        q_vocab = que_vocab\n",
    "        e_vocab = equ_vocab\n",
    "    \n",
    "    \n",
    "    decoder_input = torch.tensor([[q_vocab['<sos>'] for i in range(input_tensor.size(1))]], device = device) \n",
    "    ##[문장 길이,배치] #문장의 시작을 알리는 index를 배치 크기 만큼 입력\n",
    "\n",
    "    decoder_hidden = encoder_hidden #input_length에서 마지막 단어까지 거친 히든 [layer, batch, hidden ]\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length): #타겟 문장 전체 반복\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs) \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            for i in range(input_tensor.size(1)):\n",
    "                if topi[i].item() == q_vocab['<eos>']:\n",
    "                    continue\n",
    "                    \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721bf0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_evaluate(encoder, decoder, data_loader, que_len, criterion):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_bleu = 0\n",
    "    acc = 0\n",
    "    prefect_predict = 0\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()    \n",
    "                \n",
    "    if encoder.pretrain:\n",
    "        q_vocab = pre_vocab['que']\n",
    "        e_vocab = equ_vocab #e_vocab = pre_vocab['equ']\n",
    "    else:\n",
    "        q_vocab = que_vocab\n",
    "        e_vocab = equ_vocab\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, ans in data_loader:\n",
    "            \n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            ans = list(map(float, np.array(ans)))\n",
    "\n",
    "            \n",
    "            input_length = src.size(0)\n",
    "            target_length = tgt.size(0)\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = torch.zeros(src.size(1), que_len, encoder.hidden_size, device = device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(src[ei], encoder_hidden)\n",
    "                encoder_outputs[:, ei, :] = encoder_output[0, :]\n",
    "            \n",
    "            decoded_words = [[] for i in range(src.size(1))]\n",
    "            \n",
    "            decoder_input = torch.tensor([[q_vocab['<sos>'] for i in range(src.size(1))]], device = device) ## 처음 시작은 <sos> 토큰\n",
    "\n",
    "            loss = 0.0\n",
    "        \n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "            decoded_word = []\n",
    "            \n",
    "            squeeze_index = []\n",
    "\n",
    "            \n",
    "            for di in range(target_length): \n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs) #if attention, encoder_outputs 추가\n",
    "                topv, topi = decoder_output.topk(1) ## topi = [batch_size, 1]\n",
    "                \n",
    "                loss += criterion(decoder_output, tgt[di])\n",
    "                \n",
    "                squeeze_index.append(topi.squeeze()) ## topi.squeeze() = [batch_size, ]  squeeze_index = [[batch],[batch],[batch] ''']\n",
    "                decoder_input= topi.squeeze().detach()\n",
    "            \n",
    "            stack_squeeze = torch.stack(squeeze_index) #[length, batch]\n",
    "            \n",
    "            total_loss += (loss / target_length)           \n",
    "            \n",
    "            for batch_idx in range(src.size(1)):         ## 배치의 각 문장마다 돌면서 <eos> 토큰을 만나면 문장을 끊도록\n",
    "                a = stack_squeeze[:, batch_idx]\n",
    "                subwords = []\n",
    "                for j, k in enumerate(a):\n",
    "                    if a[j] == 3:\n",
    "                        subwords.append(\"<eos>\")\n",
    "                        decoded_word.append(subwords)\n",
    "                        break\n",
    "                    else:\n",
    "                        subwords.append(e_vocab.lookup_token(a[j])) ##eng_vocab\n",
    "                        if len(subwords) == (target_length -1):\n",
    "                            subwords.append(\"<eos>\")\n",
    "                            decoded_word.append(subwords)\n",
    "                            break\n",
    " \n",
    "            \n",
    "            real = [(\" \".join(e_vocab.lookup_tokens(tgt.T.tolist()[r])).replace(\"<pad>\", \"\")).split() for r in range(tgt.size(1))] \n",
    "            ## replace('<eos>', \"\").split()\n",
    "\n",
    "\n",
    "            pre_bleu = 0 ## 배치의 bleu score\n",
    "            \n",
    "            \n",
    "\n",
    "            for pred_idx in range(src.size(1)):\n",
    "                equation = (' '.join(decoded_word[pred_idx])).replace('<eos>',\"\")\n",
    "                try:\n",
    "                    predict_ans = eval(equation.replace('@', '//')) ## 생성된 식을 eval로 계산\n",
    "                    predict_ans = float('%.02f' % predict_ans)\n",
    "                except (SyntaxError, TypeError, ZeroDivisionError,ValueError, OverflowError):              ## 잘못된 식 생성시 에러 발생\n",
    "                    continue\n",
    "                    \n",
    "                if predict_ans == ans[pred_idx]:\n",
    "                    acc += 1\n",
    "\n",
    "                bleu_score = sentence_bleu([real[pred_idx]], decoded_word[pred_idx], smoothing_function=None, weights = (0.25,0.25,0.25,0.25))\n",
    "                \n",
    "                if bleu_score == 1.0:\n",
    "                    prefect_predict += 1 \n",
    "                pre_bleu += bleu_score\n",
    "                \n",
    "                print(f\"sentence {pred_idx}\" )\n",
    "                print(\"문장형 문제 =\", (' '.join(q_vocab.lookup_tokens(src.T.tolist()[pred_idx]))).replace(\"<pad>\", \"\").replace(\"<eos>\",\"\"))\n",
    "                print(f\"실제 수식  = \", (' '.join(real[pred_idx])))\n",
    "                print(\"실제 정답\", ans[pred_idx])\n",
    "                print(f\"예측 수식  = \", \" \".join(decoded_word[pred_idx]))\n",
    "                print(\"예측 정답\", predict_ans)\n",
    "\n",
    "                \n",
    "            \n",
    "            pre_bleu = pre_bleu / src.size(1)\n",
    "            total_bleu += pre_bleu\n",
    "            \n",
    "        print(f\"mean BLEU score : {total_bleu / len(data_loader)}, prefect_prediction : {prefect_predict} sentence, Accuracy : {acc / len(data_loader.dataset)}\")\n",
    "    \n",
    "    \n",
    "    #return total_bleu/ len(data_loader), total_loss.item() / len(data_loader) , acc / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, train_loader, max_length):\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    \n",
    "    for src, tgt, _ in train_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        loss = train(src, tgt, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "        print_loss_total += loss \n",
    "        \n",
    "    print_loss_avg = print_loss_total / len(train_loader)\n",
    "    \n",
    "    return print_loss_avg\n",
    "    \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59729893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_greedy(encoder, decoder, data_loader, que_len, criterion):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_bleu = 0\n",
    "    acc = 0\n",
    "    prefect_predict = 0\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()    \n",
    "                \n",
    "    if encoder.pretrain:\n",
    "        q_vocab = pre_vocab['que']\n",
    "        e_vocab = equ_vocab #e_vocab = pre_vocab['equ']\n",
    "    else:\n",
    "        q_vocab = que_vocab\n",
    "        e_vocab = equ_vocab\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, ans in data_loader:\n",
    "            \n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            ans = list(map(float, np.array(ans)))\n",
    "\n",
    "            \n",
    "            input_length = src.size(0)\n",
    "            target_length = tgt.size(0)\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = torch.zeros(src.size(1), que_len, encoder.hidden_size, device = device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(src[ei], encoder_hidden)\n",
    "                encoder_outputs[:, ei, :] = encoder_output[0, :]\n",
    "            \n",
    "            decoded_words = [[] for i in range(src.size(1))]\n",
    "            \n",
    "            decoder_input = torch.tensor([[q_vocab['<sos>'] for i in range(src.size(1))]], device = device) ## 처음 시작은 <sos> 토큰\n",
    "\n",
    "            loss = 0.0\n",
    "        \n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "            decoded_word = []\n",
    "            \n",
    "            squeeze_index = []\n",
    "\n",
    "            \n",
    "            for di in range(target_length): \n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs) #if attention, encoder_outputs 추가\n",
    "                topv, topi = decoder_output.topk(1) ## topi = [batch_size, 1]\n",
    "                \n",
    "                loss += criterion(decoder_output, tgt[di])\n",
    "                \n",
    "                squeeze_index.append(topi.squeeze()) ## topi.squeeze() = [batch_size, ]  squeeze_index = [[batch],[batch],[batch] ''']\n",
    "                decoder_input= topi.squeeze().detach()\n",
    "            \n",
    "            stack_squeeze = torch.stack(squeeze_index) #[length, batch]\n",
    "            \n",
    "            total_loss += (loss / target_length)           \n",
    "            \n",
    "            for batch_idx in range(src.size(1)):         ## 배치의 각 문장마다 돌면서 <eos> 토큰을 만나면 문장을 끊도록\n",
    "                a = stack_squeeze[:, batch_idx]\n",
    "                subwords = []\n",
    "                for j, k in enumerate(a):\n",
    "                    if a[j] == 3:\n",
    "                        subwords.append(\"<eos>\")\n",
    "                        decoded_word.append(subwords)\n",
    "                        break\n",
    "                    else:\n",
    "                        subwords.append(e_vocab.lookup_token(a[j])) ##eng_vocab\n",
    "                        if len(subwords) == (target_length -1):\n",
    "                            subwords.append(\"<eos>\")\n",
    "                            decoded_word.append(subwords)\n",
    "                            break\n",
    "            \n",
    "            \n",
    "            real = [(\" \".join(e_vocab.lookup_tokens(tgt.T.tolist()[r])).replace(\"<pad>\", \"\")).split() for r in range(tgt.size(1))] \n",
    "            ## replace('<eos>', \"\").split()\n",
    "\n",
    "\n",
    "            pre_bleu = 0 ## 배치의 bleu score\n",
    "            \n",
    "            \n",
    "            for pred_idx in range(src.size(1)):\n",
    "                equation = (' '.join(decoded_word[pred_idx])).replace('<eos>',\"\")\n",
    "                try:\n",
    "                    predict_ans = eval(equation.replace('@', '//')) ## 생성된 식을 eval로 계산\n",
    "                    predict_ans = float('%.02f' % predict_ans)\n",
    "                except (SyntaxError, TypeError, ZeroDivisionError,ValueError):              ## 잘못된 식 생성시 에러 발생\n",
    "                    continue\n",
    "                    \n",
    "                if predict_ans == ans[pred_idx]:\n",
    "                    acc += 1\n",
    "                    \n",
    "                bleu_score = sentence_bleu([real[pred_idx]], decoded_word[pred_idx], smoothing_function=None, weights = (0.25,0.25,0.25,0.25))\n",
    "                \n",
    "                if bleu_score == 1.0:\n",
    "                    prefect_predict += 1 \n",
    "                pre_bleu += bleu_score\n",
    "                \n",
    "            \n",
    "            pre_bleu = pre_bleu / src.size(1)\n",
    "            total_bleu += pre_bleu\n",
    "            \n",
    "    \n",
    "    \n",
    "    return total_bleu/ len(data_loader), total_loss.item() / len(data_loader) , acc / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f074806",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_len= max_length(train_json)\n",
    "valid_len = max_length(valid_json)\n",
    "test_len = 0\n",
    "que_len =max(que_len, valid_len, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_len = 98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8ee2a",
   "metadata": {},
   "source": [
    "## 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(len(pre_vocab['que']), hidden_size ,batch_size ,num_layer ,True).to(device)\n",
    "attn_decoder = LuongAttnDecoderRNN(len(equ_vocab), hidden_size, que_len, batch_size, num_layer, False).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr = 5e-4)\n",
    "decoder_optimizer = optim.Adam(attn_decoder.parameters(), lr = 5e-4)\n",
    "criterion = nn.NLLLoss(ignore_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = open('attn_loss/Lu_train_losses','w')\n",
    "b = open('attn_loss/Lu_bleu_score','w')\n",
    "v = open('attn_loss/Lu_valid_losses','w')\n",
    "a = open('attn_loss/Lu_accuracy','w')\n",
    "t.close()\n",
    "b.close()\n",
    "v.close()\n",
    "a.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47831a6f",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def start_train(encoder, attn_decoder, directory):\n",
    "    best_loss = None\n",
    "    prev_loss= 99999999\n",
    "    \n",
    "\n",
    "    \n",
    "    i = 0\n",
    "    criterion = nn.NLLLoss(ignore_index = 1)\n",
    "    for epoch in range(1, 400+1):\n",
    "        os.chdir('/data/kmkim/MWP_model')\n",
    "        t = open('attn_loss/Lu_train_losses','a')\n",
    "        b = open('attn_loss/Lu_bleu_score','a')\n",
    "        v = open('attn_loss/Lu_valid_losses','a')\n",
    "        a = open('attn_loss/Lu_accuracy','a')\n",
    "    \n",
    "        try:                \n",
    "            train_time = time.time()\n",
    "            train_loss = trainIters(encoder, attn_decoder, train_loader, que_len) ##trainiter 과 train 연관 하여서 시간이랑 loss 계산 완벽하게\n",
    "            train_end_time = time.time()\n",
    "            #train_bleu, train_loss, train_acc = new_greedy(encoder, attn_decoder, train_loader, que_len, criterion)\n",
    "            val_bleu ,val_loss, val_acc = new_greedy(encoder, attn_decoder, valid_loader, que_len, criterion)\n",
    "\n",
    "\n",
    "            t.write(str(train_loss) + '\\n')\n",
    "            b.write(str(val_bleu) + '\\n')\n",
    "            v.write(str(val_loss) + '\\n')\n",
    "            a.write(str(val_acc) + '\\n')\n",
    "        \n",
    "        finally:\n",
    "            t.close()\n",
    "            b.close()\n",
    "            v.close()\n",
    "            a.close()\n",
    "            \n",
    "        valid_end_time = time.time()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, train_time = {(train_end_time - train_time):.3f}s, train_loss = {train_loss:.4f}, \\\n",
    "        validation = {val_loss:.4f}, val_bleu = {val_bleu:.4f}, val_acc ={val_acc:.4f}, valid_time = {(valid_end_time - train_end_time):.3f}s\") \n",
    "        \n",
    "\n",
    "        if not best_loss or val_loss < best_loss:\n",
    "            os.chdir('/data/kmkim/MWP/new_problem')\n",
    "            if not os.path.isdir(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save(encoder.state_dict(), './'+ directory +'/attn_encoder.pt') \n",
    "            torch.save(attn_decoder.state_dict(), './'+directory+'/attn_decoder.pt')\n",
    "            print('model save, directory:', directory)\n",
    "\n",
    "            best_loss = val_loss\n",
    "\n",
    "            if prev_loss < val_loss:\n",
    "                i += 1\n",
    "                if i > 5:\n",
    "                    print(\"patient over 5\")\n",
    "                    break\n",
    "            else:\n",
    "                i = 0\n",
    "\n",
    "            prev_loss = val_loss\n",
    "            \n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5dbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_train(encoder, attn_decoder, 'mwp_Lu_attn_44_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3560e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_T = EncoderRNN(len(pre_vocab['que']), hidden_size ,batch_size ,num_layer, True).to(device)\n",
    "attn_decoder_T = LuongAttnDecoderRNN(len(equ_vocab), hidden_size, que_len, batch_size, num_layer, False).to(device)\n",
    "encoder_T.load_state_dict(torch.load('../saved_weights/attn_encoder.pt'))\n",
    "attn_decoder_T.load_state_dict(torch.load('../saved_weights/attn_decoder.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_csv = pd.read_csv(\"../data/math_test_1000\" , header=None)\n",
    "\n",
    "\n",
    "def data_format(df, is_stopwords = False):\n",
    "    '''\n",
    "        Args:\n",
    "            df (DataFrame) : 데이터프레임 csv\n",
    "        \n",
    "        Returns:\n",
    "            list_ (list): 데이터를 받아서 ['문장', '라벨'] 로 연결\n",
    "        \n",
    "            ex) list_= [['민혁이가 잰 직사각형의 둘레는 48m이고, 직사각형의 세로는 가로 길이의 2배 입니다. 세로는 몇 m인지 구하세요.', '((48 / 2) / (2 + 1)) * 2'],\n",
    "    '''\n",
    "    list_ = [] \n",
    "    for i in range(len(df[0])):\n",
    "        list_.append([df[0][i]])\n",
    "    for i in range(len(df[0])):    \n",
    "        list_[i].append(df[1][i])\n",
    "    for i in range(len(df[0])):\n",
    "        list_[i].append(df[2][i])\n",
    "        \n",
    "    \n",
    "    return list_\n",
    "\n",
    "test_pairs = data_format(test_csv)\n",
    "\n",
    "real_loader = DataLoader(test_pairs, batch_size = 100, shuffle = False, collate_fn = new_collate_batch, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8be9d",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f19f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import CharErrorRate\n",
    "from torchmetrics.text import WordErrorRate\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "\n",
    "wer = WordErrorRate()\n",
    "cer = CharErrorRate()\n",
    "rouge = ROUGEScore(rouge_keys = ('rougeL'))['rougeL_fmeasure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a550e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_new_evaluate(encoder, decoder, data_loader, que_len):\n",
    "    \n",
    "    total_bleu = 0\n",
    "    bleu_1gram = 0\n",
    "    bleu_2gram = 0\n",
    "    bleu_3gram = 0\n",
    "    total_cer = 0\n",
    "    total_rouge = 0\n",
    "    acc = 0\n",
    "    \n",
    "    ## 이하는 나중에 지워도 됨\n",
    "    correct= 0\n",
    "    new_bleu_acc = 0\n",
    "    data_index = []\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()    \n",
    "                \n",
    "    if encoder.pretrain:\n",
    "        q_vocab = pre_vocab['que']\n",
    "        e_vocab = equ_vocab #e_vocab = pre_vocab['equ']\n",
    "    else:\n",
    "        q_vocab = que_vocab\n",
    "        e_vocab = equ_vocab\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for loader_idx, (src, tgt, ans) in enumerate(tqdm(data_loader)):\n",
    "            \n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            ans = list(map(float, np.array(ans)))\n",
    "\n",
    "            \n",
    "            input_length = src.size(0)\n",
    "            target_length = tgt.size(0)\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = torch.zeros(src.size(1), que_len, encoder.hidden_size, device = device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(src[ei], encoder_hidden)\n",
    "                encoder_outputs[:, ei, :] = encoder_output[0, :]\n",
    "            \n",
    "            decoded_words = [[] for i in range(src.size(1))]\n",
    "            \n",
    "            decoder_input = torch.tensor([[q_vocab['<sos>'] for i in range(src.size(1))]], device = device) ## 처음 시작은 <sos> 토큰\n",
    "\n",
    "        \n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "            decoded_word = []\n",
    "            \n",
    "            squeeze_index = []\n",
    "\n",
    "            \n",
    "            for di in range(target_length): \n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs) #if attention, encoder_outputs 추가\n",
    "                topv, topi = decoder_output.topk(1) ## topi = [batch_size, 1]\n",
    "                \n",
    "                squeeze_index.append(topi.squeeze()) ## topi.squeeze() = [batch_size, ]  squeeze_index = [[batch],[batch],[batch] ''']\n",
    "                decoder_input= topi.squeeze().detach()\n",
    "            \n",
    "            stack_squeeze = torch.stack(squeeze_index) #[length, batch]\n",
    "                        \n",
    "            for batch_idx in range(src.size(1)):         ## 배치의 각 문장마다 돌면서 <eos> 토큰을 만나면 문장을 끊도록\n",
    "                a = stack_squeeze[:, batch_idx]\n",
    "                subwords = []\n",
    "                for j, k in enumerate(a):\n",
    "                    if a[j] == 3:\n",
    "                        subwords.append(\"<eos>\")\n",
    "                        decoded_word.append(subwords)\n",
    "                        break\n",
    "                    else:\n",
    "                        subwords.append(e_vocab.lookup_token(a[j])) ##eng_vocab\n",
    "                        if len(subwords) == (target_length -1):\n",
    "                            subwords.append(\"<eos>\")\n",
    "                            decoded_word.append(subwords)\n",
    "                            break\n",
    " \n",
    "            \n",
    "            real = [(\" \".join(e_vocab.lookup_tokens(tgt.T.tolist()[r])).replace(\"<pad>\", \"\")).split() for r in range(tgt.size(1))] \n",
    "            ## replace('<eos>', \"\").split()\n",
    "    \n",
    "\n",
    "            for pred_idx in range(src.size(1)):\n",
    "                equation = (' '.join(decoded_word[pred_idx])).replace('<eos>',\"\")\n",
    "                real_equ = (\" \".join(equ_vocab.lookup_tokens(tgt.T.tolist()[pred_idx]))).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "\n",
    "                try:\n",
    "                    predict_ans = eval(equation.replace('@', '//')) ## 생성된 식을 eval로 계산\n",
    "                    predict_ans = float('%.02f' % predict_ans)\n",
    "                except (SyntaxError, TypeError, ZeroDivisionError,ValueError, OverflowError):              ## 잘못된 식 생성시 에러 발생\n",
    "                    continue\n",
    "                    \n",
    "                if predict_ans == ans[pred_idx]:\n",
    "                    acc += 1\n",
    "                \n",
    "                \n",
    "                real = real_equ.split()\n",
    "                real.extend([\"<eos>\"])\n",
    "                \n",
    "                \n",
    "                total_bleu += sentence_bleu([real], decoded_word[pred_idx]) # SmoothingFunction().method6\n",
    "                bleu_1gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(1.0)])\n",
    "                bleu_2gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(0.5,0.5)])\n",
    "                bleu_3gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(0.333,0.333,0.334)])                \n",
    "\n",
    "                total_cer += cer([\" \".join(decoded_word[pred_idx])],[' '.join(real)]).item()\n",
    "                total_rouge += rouge([\" \".join(decoded_word[pred_idx])], [' '.join(real)]).item()\n",
    "                \n",
    "                \n",
    "                #if ans[pred_idx] == predict_ans:\n",
    "                if (loader_idx == len(data_loader)-1) and (batch_idx == src.size(1) - 1):\n",
    "                    \n",
    "                    print(f\"sentence {pred_idx}\" )\n",
    "                    print(\"문장형 문제 =\", (' '.join(pre_vocab['que'].lookup_tokens(src.T.tolist()[pred_idx]))).replace(\"<pad>\", \"\").replace(\"<eos>\",\"\"))\n",
    "                    print(f\"실제 수식  = \", (' '.join(real)))\n",
    "                    print(\"실제 정답\", ans[pred_idx])\n",
    "                    print(f\"예측 수식  = \", \" \".join(decoded_word[pred_idx]))\n",
    "                    print(\"예측 정답\", predict_ans)\n",
    "                \n",
    "            \n",
    "\n",
    "    #return total_bleu/ len(data_loader), total_loss.item() / len(data_loader) , acc / len(data_loader.dataset)\n",
    "    return total_bleu / (len(data_loader) * batch_size), acc / (len(data_loader) * batch_size), bleu_1gram / (len(data_loader) * batch_size), bleu_2gram / (len(data_loader) * batch_size), bleu_3gram / (len(data_loader) * batch_size), total_cer / (len(data_loader) * batch_size),total_rouge / (len(data_loader) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_evaluate(encoder_T, attn_decoder_T, test_loader, que_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04511b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_new_test(encoder, decoder, data_loader, que_len):\n",
    "    \n",
    "    total_bleu = 0\n",
    "    bleu_1gram = 0\n",
    "    bleu_2gram = 0\n",
    "    bleu_3gram = 0\n",
    "    total_cer = 0\n",
    "    total_rouge = 0\n",
    "    error_count = 0\n",
    "    acc = 0\n",
    "    \n",
    "    ## 이하는 나중에 지워도 됨\n",
    "    correct= 0\n",
    "    new_bleu_acc = 0\n",
    "    data_index = []\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()    \n",
    "                \n",
    "    if encoder.pretrain:\n",
    "        q_vocab = pre_vocab['que']\n",
    "        e_vocab = equ_vocab #e_vocab = pre_vocab['equ']\n",
    "    else:\n",
    "        q_vocab = que_vocab\n",
    "        e_vocab = equ_vocab\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for loader_idx, (src, tgt, ans) in enumerate(tqdm(data_loader)):\n",
    "            \n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            ans = list(map(float, np.array(ans)))\n",
    "\n",
    "            \n",
    "            input_length = src.size(0)\n",
    "            target_length = tgt.size(0)\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = torch.zeros(src.size(1), que_len, encoder.hidden_size, device = device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(src[ei], encoder_hidden)\n",
    "                encoder_outputs[:, ei, :] = encoder_output[0, :]\n",
    "            \n",
    "            decoded_words = [[] for i in range(src.size(1))]\n",
    "            \n",
    "            decoder_input = torch.tensor([[q_vocab['<sos>'] for i in range(src.size(1))]], device = device) ## 처음 시작은 <sos> 토큰\n",
    "\n",
    "        \n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "            decoded_word = []\n",
    "            \n",
    "            squeeze_index = []\n",
    "\n",
    "            \n",
    "            for di in range(target_length): \n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs) #if attention, encoder_outputs 추가\n",
    "                topv, topi = decoder_output.topk(1) ## topi = [batch_size, 1]\n",
    "                \n",
    "                squeeze_index.append(topi.squeeze()) ## topi.squeeze() = [batch_size, ]  squeeze_index = [[batch],[batch],[batch] ''']\n",
    "                decoder_input= topi.squeeze().detach()\n",
    "            \n",
    "            stack_squeeze = torch.stack(squeeze_index) #[length, batch]\n",
    "                        \n",
    "            for batch_idx in range(src.size(1)):         ## 배치의 각 문장마다 돌면서 <eos> 토큰을 만나면 문장을 끊도록\n",
    "                a = stack_squeeze[:, batch_idx]\n",
    "                subwords = []\n",
    "                for j, k in enumerate(a):\n",
    "                    if a[j] == 3:\n",
    "                        subwords.append(\"<eos>\")\n",
    "                        decoded_word.append(subwords)\n",
    "                        break\n",
    "                    else:\n",
    "                        subwords.append(e_vocab.lookup_token(a[j])) ##eng_vocab\n",
    "                        if len(subwords) == (target_length -1):\n",
    "                            subwords.append(\"<eos>\")\n",
    "                            decoded_word.append(subwords)\n",
    "                            break\n",
    " \n",
    "            \n",
    "            real = [(\" \".join(e_vocab.lookup_tokens(tgt.T.tolist()[r])).replace(\"<pad>\", \"\")).split() for r in range(tgt.size(1))] \n",
    "            ## replace('<eos>', \"\").split()\n",
    "    \n",
    "\n",
    "            for pred_idx in range(src.size(1)):\n",
    "                equation = (' '.join(decoded_word[pred_idx])).replace('<eos>',\"\")\n",
    "                real_equ = (\" \".join(equ_vocab.lookup_tokens(tgt.T.tolist()[pred_idx]))).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "\n",
    "                try:\n",
    "                    predict_ans = eval(equation.replace('@', '//')) ## 생성된 식을 eval로 계산\n",
    "                    predict_ans = float('%.02f' % predict_ans)\n",
    "                except (SyntaxError, TypeError, ZeroDivisionError,ValueError, OverflowError):              ## 잘못된 식 생성시 에러 발생\n",
    "                    continue\n",
    "\n",
    "                real = real_equ.split()\n",
    "                real.extend([\"<eos>\"])\n",
    "                \n",
    "                \n",
    "                if ans[pred_idx] == predict_ans:\n",
    "                    correct += 1\n",
    "                    print(\"***********정답******************\")\n",
    "                    print(f\"데이터 몇번째 :{loader_idx * 10 + pred_idx +1} \") ## 10 = real_dataloader batch size\n",
    "                    data_index.append(loader_idx * 10 + pred_idx +1)\n",
    "                    print(\"문장형 문제 =\", (' '.join(pre_vocab['que'].lookup_tokens(src.T.tolist()[pred_idx]))).replace(\"<pad>\", \"\").replace(\"<eos>\",\"\"))\n",
    "                    print(f\"실제 수식  = \", (' '.join(real)))\n",
    "                    print(f\"예측 수식  = \", \" \".join(decoded_word[pred_idx]))\n",
    "                    print(\"실제 정답\", ans[pred_idx])\n",
    "                    print(\"예측 정답\", predict_ans)\n",
    "                    bleu_score_acc = sentence_bleu([real], decoded_word[pred_idx],smoothing_function =None)\n",
    "                    \n",
    "                    bleu_1gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(1.0)])\n",
    "                    bleu_2gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(0.5,0.5)])\n",
    "                    bleu_3gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(0.333,0.333,0.334)])\n",
    "                    total_cer += wer([\" \".join(decoded_word[pred_idx])],[' '.join(real)]).item()\n",
    "                    total_rouge += rouge([\" \".join(decoded_word[pred_idx])], [' '.join(real)]).item()                  \n",
    "                    #total_cer += cer(real, decoded_words[pred_idx]).item()\n",
    "                    new_bleu_acc += bleu_score_acc\n",
    "                    \n",
    "                    print(\"*********************************\")\n",
    "       \n",
    "                else:\n",
    "                    if error_count < 247:\n",
    "                        print(\"^^^^^^^^^^오답^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        error_count += 1\n",
    "                        print(\"문장형 문제 =\", (' '.join(pre_vocab['que'].lookup_tokens(src.T.tolist()[pred_idx]))).replace(\"<pad>\", \"\").replace(\"<eos>\",\"\"))\n",
    "                        print(f\"실제 수식  = \", (' '.join(real)))\n",
    "                        print(f\"예측 수식  = \", \" \".join(decoded_word[pred_idx]))\n",
    "                        print(\"실제 정답\", ans[pred_idx])\n",
    "                        print(\"예측 정답\", predict_ans)\n",
    "                        bleu_score_acc = sentence_bleu([real], decoded_word[pred_idx],smoothing_function =None)\n",
    "\n",
    "                        bleu_1gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(1.0)])\n",
    "                        bleu_2gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(0.5,0.5)])\n",
    "                        bleu_3gram += sentence_bleu([real], decoded_word[pred_idx], weights = [(0.333,0.333,0.334)])\n",
    "                        total_cer += wer([\" \".join(decoded_word[pred_idx])],[' '.join(real)]).item()\n",
    "                        total_rouge += rouge([\" \".join(decoded_word[pred_idx])], [' '.join(real)]).item()\n",
    "                        new_bleu_acc += bleu_score_acc\n",
    "    \n",
    "        print(f\"total 300/ correct:{correct/311} \")\n",
    "        print(f\"correct : {correct}\")\n",
    "        print(f\"data index\",data_index)\n",
    "        print(f\"total 300 bleu :{new_bleu_acc/ 311}\")\n",
    "        print(f\"total 300 bleu_1 :{bleu_1gram/ 311}\")\n",
    "        print(f\"total 300 bleu_2 :{bleu_2gram/ 311}\")\n",
    "        print(f\"total 300 bleu_3:{bleu_3gram/ 311}\")\n",
    "        print(f\"total cer :{total_cer}\")\n",
    "        print(f\"total 300 cer :{total_cer/ 311}\")\n",
    "        print(f\"total 300 rouge :{total_rouge/ 311}\")\n",
    "\n",
    "        print(f\"error count :{error_count}\")\n",
    "        \n",
    "\n",
    "                    \n",
    "                    \n",
    "                \n",
    "\n",
    "             \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fefcf",
   "metadata": {},
   "source": [
    "## 실제 데이터 세트로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_T = EncoderRNN(len(pre_vocab['que']), hidden_size ,100,num_layer, True).to(device)\n",
    "attn_decoder_T = LuongAttnDecoderRNN(len(equ_vocab), hidden_size, que_len, 100, num_layer, False).to(device)\n",
    "encoder_T.load_state_dict(torch.load('../saved_weights/attn_encoder.pt'))\n",
    "attn_decoder_T.load_state_dict(torch.load('../saved_weights/attn_decoder.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_test(encoder_T, attn_decoder_T, real_loader, que_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2014b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
